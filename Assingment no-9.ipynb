{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assingment no-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "# engineering in depth.\n",
    "# Ans=\n",
    "Feature engineering is the process of transforming raw data into meaningful features that can be used by machine learning algorithms to improve model performance. It involves creating, selecting, and transforming features from the available data to represent the underlying patterns and relationships.\n",
    "\n",
    "The aspects of feature engineering include:\n",
    "\n",
    "Feature Creation: This involves creating new features from the existing data. It can be done through mathematical operations, combining multiple variables, or extracting relevant information. For example, in image recognition, new features can be created by extracting color histograms or texture features from images.\n",
    "\n",
    "Feature Selection: This involves selecting the most informative features that are relevant to the target variable. It helps to reduce dimensionality, improve model interpretability, and reduce the risk of overfitting. Feature selection techniques include statistical tests, correlation analysis, and model-based methods like Lasso regression.\n",
    "\n",
    "Feature Transformation: This involves transforming the data to improve its representation or meet certain assumptions of the machine learning algorithms. It includes techniques like scaling or normalization to bring features to a similar scale, logarithmic or power transformations to handle skewed distributions, and encoding categorical variables into numerical representations.\n",
    "\n",
    "Handling Missing Values: Missing values in the data can be a common issue, and feature engineering involves dealing with missing values appropriately. It can include imputing missing values using techniques like mean, median, or regression imputation, or creating new features to indicate the presence or absence of missing values.\n",
    "\n",
    "Feature Encoding: Categorical variables need to be encoded into numerical representations to be used in machine learning models. Common encoding techniques include one-hot encoding, label encoding, and ordinal encoding.\n",
    "\n",
    "Feature Scaling: Scaling numerical features to a similar range can help algorithms converge faster and prevent certain features from dominating others. Scaling techniques include standardization (mean of 0 and standard deviation of 1) and normalization (scaling to a range of 0 to 1).\n",
    "\n",
    "Feature Extraction: This involves extracting the most relevant information from the data using techniques like Principal Component Analysis (PCA), Singular Value Decomposition (SVD), or Non-Negative Matrix Factorization (NMF). It helps to reduce dimensionality and capture the most important features.\n",
    "\n",
    "Effective feature engineering requires domain knowledge, an understanding of the data, and iterative experimentation to identify the features that have the most predictive power and improve model performance. It is an essential step in the machine learning pipeline and can significantly impact the success of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "# methods of function selection?\n",
    "# Ans=\n",
    "Feature selection is the process of selecting a subset of relevant features from the available set of features in a dataset. The aim of feature selection is to improve model performance by reducing the dimensionality of the data, improving model interpretability, reducing training time, and mitigating the risk of overfitting.\n",
    "\n",
    "There are several methods for feature selection:\n",
    "\n",
    "Filter Methods: These methods rank features based on statistical measures like correlation, mutual information, or chi-square test. Features are evaluated independently of the chosen machine learning algorithm. Common filter methods include Pearson's correlation coefficient, Information Gain, and ANOVA F-value.\n",
    "\n",
    "Wrapper Methods: These methods select features based on how well they improve the performance of a specific machine learning algorithm. It involves training and evaluating the model using different feature subsets. Recursive Feature Elimination (RFE) and Forward/Backward Stepwise Selection are examples of wrapper methods.\n",
    "\n",
    "Embedded Methods: These methods incorporate feature selection as part of the model training process. They select features during the model's training phase based on built-in feature importance measures. Examples include Lasso regression, Ridge regression, and decision tree-based algorithms like Random Forest and Gradient Boosting.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated features called principal components. It selects components based on their variance, aiming to capture the maximum amount of information in fewer dimensions.\n",
    "\n",
    "Regularization: Regularization methods like L1 (Lasso) and L2 (Ridge) regularization can be used to penalize the coefficients of less important features, effectively selecting the most relevant ones. These methods are commonly used in linear regression and logistic regression models.\n",
    "\n",
    "The choice of feature selection method depends on the dataset, the characteristics of the features, the machine learning algorithm being used, and the goals of the analysis. It is often recommended to try different methods and evaluate their impact on model performance to determine the most effective feature subset for a particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "# approach?\n",
    "# Ans=\n",
    "Function selection approaches are used in feature selection to identify the most relevant subset of features for a particular machine learning task. The two commonly used approaches are filter methods and wrapper methods.\n",
    "\n",
    "Filter Methods:\n",
    "\n",
    "Filter methods rank features based on their relevance to the target variable, using statistical measures such as correlation, mutual information, or chi-square test.\n",
    "These methods evaluate the features independently of the chosen machine learning algorithm.\n",
    "Pros:\n",
    "Computationally efficient, as they do not require training the machine learning model.\n",
    "Provide an initial assessment of feature importance.\n",
    "Can handle large datasets with high dimensionality.\n",
    "Cons:\n",
    "May not consider the interactions between features.\n",
    "Cannot account for the specific needs of the machine learning algorithm.\n",
    "Wrapper Methods:\n",
    "\n",
    "Wrapper methods select features based on how well they improve the performance of a specific machine learning algorithm.\n",
    "These methods involve training and evaluating the model using different feature subsets.\n",
    "Pros:\n",
    "Can capture the interactions between features and the specific requirements of the machine learning algorithm.\n",
    "Can lead to better model performance by considering feature combinations.\n",
    "Can handle complex relationships between features and the target variable.\n",
    "Cons:\n",
    "Computationally expensive, as they require training and evaluating the model multiple times for different feature subsets.\n",
    "Prone to overfitting if the dataset is small or if the search space is large.\n",
    "May not be suitable for high-dimensional datasets due to the curse of dimensionality.\n",
    "The choice between filter and wrapper methods depends on the specific task and the available computational resources. Filter methods are generally faster and can provide an initial feature ranking, making them useful for quick feature selection. Wrapper methods tend to yield more accurate results but at a higher computational cost. It is often recommended to combine both approaches to get a comprehensive understanding of feature importance and select the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "# i. Describe the overall feature selection process.\n",
    "\n",
    "# ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "# widely used function extraction algorithms?\n",
    "# Ans=\n",
    "i. The overall feature selection process typically involves the following steps:\n",
    "\n",
    "Data Preparation: Preprocess the dataset by handling missing values, outliers, and data normalization or scaling if needed.\n",
    "\n",
    "Feature Evaluation: Evaluate the importance or relevance of each feature to the target variable. This can be done using statistical measures, correlation analysis, or domain knowledge.\n",
    "\n",
    "Feature Ranking: Rank the features based on their importance or relevance scores obtained from the evaluation step.\n",
    "\n",
    "Feature Selection: Select the top-ranked features based on a predefined threshold or a specific number of desired features. This can be done using filter methods, wrapper methods, or a combination of both.\n",
    "\n",
    "Model Training and Evaluation: Train a machine learning model using the selected features and evaluate its performance on a separate test dataset. This step helps assess the impact of feature selection on the model's accuracy and generalization ability.\n",
    "\n",
    "Iterative Refinement: Iterate through the feature selection process, re-evaluating and selecting features, and retraining the model if necessary, until the desired performance is achieved.\n",
    "\n",
    "ii. The key underlying principle of feature extraction is to transform the original features into a new set of features that better represent the data while reducing its dimensionality. This is done by combining or transforming the existing features to extract more meaningful and informative representations.\n",
    "\n",
    "For example, in the context of image recognition, a common feature extraction algorithm is the Convolutional Neural Network (CNN). A CNN consists of multiple convolutional layers that learn to extract relevant visual features from images. Each layer applies a set of learnable filters to the input image, detecting different patterns or textures. The subsequent layers combine these lower-level features to form higher-level representations, capturing more abstract concepts in the images. The final layer of the CNN extracts the most informative features and feeds them into a classifier for the task of image classification.\n",
    "\n",
    "Other widely used feature extraction algorithms include Principal Component Analysis (PCA) and Independent Component Analysis (ICA), which aim to find orthogonal representations that capture the most significant variations in the data. These algorithms project the original features onto a new space where the new features are linear combinations of the original features, emphasizing the most important dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "# Ans=\n",
    "The feature engineering process in the context of text categorization involves transforming raw text data into numerical features that can be used by machine learning algorithms. Here is a step-by-step description of the feature engineering process for text categorization:\n",
    "\n",
    "Text Preprocessing: This step involves cleaning and preprocessing the raw text data. It typically includes tasks such as removing punctuation, converting text to lowercase, removing stop words (common words like \"the,\" \"is,\" etc.), and handling special characters or symbols.\n",
    "\n",
    "Tokenization: Tokenization is the process of breaking the text into individual words or tokens. This step involves splitting the text into meaningful units, such as words or n-grams (sequences of consecutive words), to represent the text's structure.\n",
    "\n",
    "Feature Extraction: In this step, the tokens are converted into numerical features that can be used for machine learning. Some common feature extraction techniques for text categorization include:\n",
    "\n",
    "Bag-of-Words (BoW): In BoW representation, each document is represented as a vector where each dimension corresponds to a unique word in the corpus. The value of each dimension represents the frequency of that word in the document.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF is a weighting scheme that takes into account the importance of a word not only within a document but also across the entire corpus. It assigns higher weights to words that appear frequently in a document but less frequently in other documents.\n",
    "\n",
    "Word Embeddings: Word embeddings are dense vector representations that capture the semantic meaning of words. Techniques like Word2Vec or GloVe can be used to learn word embeddings from large text corpora. These embeddings can then be used as features for text categorization tasks.\n",
    "\n",
    "Feature Selection: Once the features are extracted, feature selection techniques can be applied to select the most informative features. This helps reduce the dimensionality of the feature space and improves the model's efficiency and performance. Techniques like chi-square test, mutual information, or L1 regularization can be used for feature selection.\n",
    "\n",
    "Model Training and Evaluation: With the selected features, a machine learning model (such as a classifier) is trained on the labeled data. The model is then evaluated using appropriate evaluation metrics such as accuracy, precision, recall, or F1-score to assess its performance.\n",
    "\n",
    "Iterative Refinement: The feature engineering process may involve an iterative refinement step where different feature engineering techniques, such as different tokenization approaches or feature selection methods, are experimented with to improve the model's performance. This step may also include domain-specific knowledge and fine-tuning based on the specific requirements of the text categorization problem.\n",
    "\n",
    "By performing these steps, the feature engineering process transforms the raw text data into a suitable representation that captures the relevant information needed for accurate text categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "# two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "# cosine.\n",
    "# Ans=\n",
    "Cosine similarity is a commonly used metric for text categorization because it captures the similarity between two documents based on their content, regardless of their length. It measures the cosine of the angle between two vectors, which represents the similarity between the vectors' directions.\n",
    "\n",
    "In text categorization, documents are often represented as high-dimensional vectors, where each dimension corresponds to a specific term or word. The values in the vectors represent the frequency or importance of the terms in the documents. Cosine similarity calculates the cosine of the angle between these document vectors, which indicates how similar the documents are in terms of their term frequencies or importance.\n",
    "\n",
    "The formula to calculate cosine similarity between two vectors A and B is as follows:\n",
    "\n",
    "cosine_similarity(A, B) = (A dot B) / (||A|| * ||B||)\n",
    "\n",
    "Where:\n",
    "\n",
    "A dot B is the dot product of vectors A and B\n",
    "||A|| and ||B|| are the Euclidean norms (lengths) of vectors A and B\n",
    "Using the given document-term matrix:\n",
    "\n",
    "Document 1: (2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
    "Document 2: (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
    "\n",
    "To calculate the cosine similarity, we first compute the dot product and the Euclidean norms:\n",
    "\n",
    "Dot product = 22 + 31 + 20 + 00 + 23 + 32 + 31 + 03 + 1*1 = 24\n",
    "||A|| = sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) = sqrt(34)\n",
    "||B|| = sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) = sqrt(29)\n",
    "\n",
    "Now we can calculate the cosine similarity:\n",
    "\n",
    "cosine_similarity = 24 / (sqrt(34) * sqrt(29))\n",
    "\n",
    "The resemblance in cosine is the value of the cosine similarity, which is approximately 0.966.\n",
    "\n",
    "Therefore, the resemblance in cosine between the two rows in the document-term matrix is 0.966. This indicates a relatively high similarity between the two documents based on their term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.\n",
    "\n",
    "# i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "# calculate the Hamming gap.\n",
    "\n",
    "# ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "# 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "# Ans=\n",
    "i. The Hamming distance is a measure of dissimilarity between two strings of equal length. It calculates the number of positions at which the corresponding elements in the strings are different.\n",
    "\n",
    "To calculate the Hamming distance, we compare each corresponding pair of elements in the strings and count the number of positions where they differ.\n",
    "\n",
    "Given:\n",
    "String A: 10001011\n",
    "String B: 11001111\n",
    "\n",
    "To calculate the Hamming distance:\n",
    "Hamming distance = number of positions where A and B differ\n",
    "\n",
    "In this case, the positions where A and B differ are the 3rd and 4th positions.\n",
    "\n",
    "Therefore, the Hamming distance between 10001011 and 11001111 is 2.\n",
    "\n",
    "ii. The Jaccard index and the similarity matching coefficient are similarity measures used to compare the overlap between two sets or binary vectors.\n",
    "\n",
    "Given:\n",
    "Set A: (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "Set B: (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "To calculate the Jaccard index, we divide the size of the intersection of sets A and B by the size of their union:\n",
    "Jaccard index = |A ∩ B| / |A ∪ B|\n",
    "\n",
    "To calculate the similarity matching coefficient, we divide the size of the intersection of sets A and B by the size of set A:\n",
    "Similarity matching coefficient = |A ∩ B| / |A|\n",
    "\n",
    "In this case:\n",
    "Intersection of A and B: (1, 0, 0, 0, 1, 0, 0, 1)\n",
    "Union of A and B: (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "Size of intersection: 6\n",
    "Size of union: 8\n",
    "Size of A: 8\n",
    "\n",
    "Calculating the Jaccard index:\n",
    "Jaccard index = 6 / 8 = 0.75\n",
    "\n",
    "Calculating the similarity matching coefficient:\n",
    "Similarity matching coefficient = 6 / 8 = 0.75\n",
    "\n",
    "Therefore, the Jaccard index and similarity matching coefficient between the two sets are both 0.75, indicating a high degree of similarity or overlap between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "# What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "# What can be done about it?\n",
    "# Ans=\n",
    "A high-dimensional data set refers to a data set that contains a large number of features or dimensions. In other words, it has a large number of variables that describe each data point or observation. The number of dimensions or features is typically much larger than the number of samples or data points in the set.\n",
    "\n",
    "Real-life examples of high-dimensional data sets include:\n",
    "\n",
    "Image data: Images are often represented as high-dimensional data sets, with each pixel being a feature. High-resolution images can have millions of pixels, resulting in high-dimensional representations.\n",
    "\n",
    "Genomic data: Genomic data, such as DNA sequences or gene expression data, can have thousands or even millions of features representing different genes or genomic regions.\n",
    "\n",
    "Text data: Text data can be transformed into high-dimensional representations using techniques like bag-of-words or word embeddings. Each unique word or token in the text becomes a feature.\n",
    "\n",
    "Using machine learning techniques on high-dimensional data sets poses several challenges:\n",
    "\n",
    "Curse of dimensionality: As the number of dimensions increases, the data becomes more sparse in the high-dimensional space. This can lead to overfitting and difficulties in finding meaningful patterns or relationships in the data.\n",
    "\n",
    "Increased computational complexity: The computational resources required to train and evaluate models on high-dimensional data can be significant. Algorithms may become computationally expensive or even infeasible to run.\n",
    "\n",
    "Increased risk of noise and irrelevant features: With a large number of dimensions, there is a higher chance of including irrelevant or noisy features, which can hinder the model's performance and interpretability.\n",
    "\n",
    "To address these challenges, various techniques can be employed:\n",
    "\n",
    "Feature selection: Identifying and selecting the most relevant features can reduce the dimensionality of the data and improve model performance. Techniques like statistical tests, correlation analysis, or regularization methods can be used for feature selection.\n",
    "\n",
    "Dimensionality reduction: Techniques such as Principal Component Analysis (PCA) or t-SNE (t-Distributed Stochastic Neighbor Embedding) can be used to reduce the dimensionality of the data while preserving important patterns and relationships.\n",
    "\n",
    "Regularization: Regularization techniques, such as L1 or L2 regularization, can help in controlling the complexity of the models and prevent overfitting in high-dimensional settings.\n",
    "\n",
    "Model selection and evaluation: It is important to choose models that are suitable for high-dimensional data, such as sparse models or models specifically designed for feature selection. Model performance should be evaluated using appropriate metrics, considering the challenges associated with high-dimensional data.\n",
    "\n",
    "Overall, dealing with high-dimensional data requires careful consideration of the data characteristics, appropriate feature engineering techniques, and the selection of suitable machine learning algorithms and evaluation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Make a few quick notes on:\n",
    "\n",
    "# PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "# 2. Use of vectors\n",
    "\n",
    "# 3. Embedded technique\n",
    "\n",
    "# 10. Make a comparison between:\n",
    "\n",
    "# 1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "# 2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "# 3. SMC vs. Jaccard coefficient\n",
    "# Ans=\n",
    "PCA (Principal Component Analysis):\n",
    "PCA stands for Principal Component Analysis.\n",
    "It is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space.\n",
    "PCA identifies the principal components, which are orthogonal directions that capture the maximum variance in the data.\n",
    "It helps in visualizing and understanding the underlying structure of the data and can be used for feature extraction.\n",
    "Use of vectors:\n",
    "Vectors are mathematical objects that represent quantities with both magnitude and direction.\n",
    "In the context of machine learning, vectors are commonly used to represent data points or features.\n",
    "Vectors can be represented as arrays or lists of numbers, where each number corresponds to a specific dimension or feature.\n",
    "They are used in various machine learning algorithms, such as clustering, classification, and regression, to represent and manipulate data.\n",
    "Embedded technique:\n",
    "Embedded techniques refer to feature selection methods that are incorporated within the learning algorithm itself.\n",
    "Unlike filter and wrapper approaches that are applied independently of the learning algorithm, embedded techniques consider feature selection as part of the model building process.\n",
    "Embedded techniques can automatically select relevant features or learn feature importance while training the model.\n",
    "Examples of embedded techniques include regularization methods like Lasso (L1) and Ridge (L2) regression, decision tree-based feature importance, and algorithms with built-in feature selection mechanisms.\n",
    "Sequential backward exclusion vs. sequential forward selection:\n",
    "Sequential backward exclusion and sequential forward selection are both feature selection algorithms used to iteratively select a subset of features.\n",
    "Sequential backward exclusion starts with all features and removes one feature at a time based on a predefined criterion until the desired number of features is reached.\n",
    "Sequential forward selection starts with an empty set of features and adds one feature at a time based on a predefined criterion until the desired number of features is reached.\n",
    "The main difference is the direction of selection: backward exclusion starts with all features and removes them, while forward selection starts with no features and adds them.\n",
    "Both methods aim to find the optimal subset of features that maximizes the performance of the model.\n",
    "Filter vs. wrapper function selection methods:\n",
    "Filter methods for feature selection evaluate the relevance of features based on their individual characteristics, such as correlation with the target variable or statistical tests.\n",
    "These methods assess the features independently of the learning algorithm and select the top-ranked features.\n",
    "Wrapper methods, on the other hand, evaluate the quality of feature subsets by training and testing the learning algorithm on different feature combinations.\n",
    "They consider the performance of the learning algorithm as a whole and search for an optimal subset of features that maximizes the model's performance.\n",
    "Wrapper methods tend to be more computationally expensive than filter methods but can potentially find more accurate feature subsets.\n",
    "SMC (Simple Matching Coefficient) vs. Jaccard coefficient:\n",
    "SMC and Jaccard coefficient are similarity measures used to compare sets of binary features.\n",
    "SMC measures the proportion of agreement between two sets, i.e., the number of matching features divided by the total number of features.\n",
    "Jaccard coefficient measures the proportion of shared features between two sets, i.e., the number of common features divided by the total number of unique features in both sets.\n",
    "SMC is suitable for comparing sets with equal importance for each feature, while the Jaccard coefficient is more suitable for comparing sets where the presence or absence of features is important.\n",
    "Both measures range from 0 to 1, where 0 indicates no similarity and 1 indicates perfect similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
