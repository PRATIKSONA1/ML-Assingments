{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assingment no-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "# and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "# a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "# b) For each set of centroid values, calculate the SSE.\n",
    "# Ans=\n",
    "To perform the k-means clustering with the given data points and centroids, we will follow the steps of the algorithm.\n",
    "\n",
    "a) K-means Clustering:\n",
    "\n",
    "Initialize the centroids:\n",
    "Set 1: Centroid 1 = 15, Centroid 2 = 32\n",
    "Set 2: Centroid 1 = 12, Centroid 2 = 30\n",
    "\n",
    "Assign data points to clusters:\n",
    "\n",
    "Calculate the Euclidean distance between each data point and both centroids.\n",
    "Assign each data point to the cluster with the closest centroid.\n",
    "Set 1:\n",
    "Data point 5: Distance to Centroid 1 = |5 - 15| = 10, Distance to Centroid 2 = |5 - 32| = 27\n",
    "Data point 10: Distance to Centroid 1 = |10 - 15| = 5, Distance to Centroid 2 = |10 - 32| = 22\n",
    "Data point 15: Distance to Centroid 1 = 0, Distance to Centroid 2 = |15 - 32| = 17\n",
    "Data point 20: Distance to Centroid 1 = |20 - 15| = 5, Distance to Centroid 2 = |20 - 32| = 12\n",
    "Data point 25: Distance to Centroid 1 = |25 - 15| = 10, Distance to Centroid 2 = |25 - 32| = 7\n",
    "Data point 30: Distance to Centroid 1 = |30 - 15| = 15, Distance to Centroid 2 = |30 - 32| = 2\n",
    "Data point 35: Distance to Centroid 1 = |35 - 15| = 20, Distance to Centroid 2 = |35 - 32| = 3\n",
    "\n",
    "Assignments:\n",
    "Cluster 1: {15, 20, 25, 30}\n",
    "Cluster 2: {5, 10, 35}\n",
    "\n",
    "Set 2:\n",
    "Data point 5: Distance to Centroid 1 = |5 - 12| = 7, Distance to Centroid 2 = |5 - 30| = 25\n",
    "Data point 10: Distance to Centroid 1 = |10 - 12| = 2, Distance to Centroid 2 = |10 - 30| = 20\n",
    "Data point 15: Distance to Centroid 1 = |15 - 12| = 3, Distance to Centroid 2 = |15 - 30| = 15\n",
    "Data point 20: Distance to Centroid 1 = |20 - 12| = 8, Distance to Centroid 2 = |20 - 30| = 10\n",
    "Data point 25: Distance to Centroid 1 = |25 - 12| = 13, Distance to Centroid 2 = |25 - 30| = 5\n",
    "Data point 30: Distance to Centroid 1 = |30 - 12| = 18, Distance to Centroid 2 = |30 - 30| = 0\n",
    "Data point 35: Distance to Centroid 1 = |35 - 12| = 23, Distance to Centroid 2 = |35 - 30| = 5\n",
    "\n",
    "Assignments:\n",
    "Cluster 1: {10, 15, 20, 25}\n",
    "Cluster 2: {5, 30, 35}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "# Ans=\n",
    "Market Basket Analysis (also known as association analysis) is a technique used in retail and marketing to uncover relationships between products that are frequently purchased together. It leverages the concept of association rules to identify patterns and associations among items in a transactional dataset, such as customer shopping baskets.\n",
    "\n",
    "The process of Market Basket Analysis involves the following steps:\n",
    "\n",
    "Data Collection: Transactional data is collected, typically in the form of purchase records or shopping baskets, where each transaction represents a customer's purchase.\n",
    "\n",
    "Data Preprocessing: The transactional data is cleaned and prepared for analysis. This may involve removing duplicates, handling missing values, and transforming the data into a suitable format for association analysis.\n",
    "\n",
    "Item Frequency Calculation: The frequency of occurrence of individual items in the dataset is calculated. This helps identify popular items and understand their importance in the market.\n",
    "\n",
    "Association Rule Generation: Association rules are generated based on the concept of support, confidence, and lift. Support measures the frequency of co-occurrence of items in transactions, confidence measures the conditional probability of purchasing one item given the presence of another, and lift measures the strength of association between items.\n",
    "\n",
    "Rule Evaluation and Selection: The generated association rules are evaluated based on predefined thresholds or metrics. Rules that meet the desired criteria, such as minimum support and confidence thresholds, are selected for further analysis.\n",
    "\n",
    "Interpretation and Action: The selected association rules are interpreted to gain insights into customer behavior and purchasing patterns. These insights can be used to make informed business decisions, such as product placement, cross-selling, bundling, and targeted marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Give an example of the Apriori algorithm for learning association rules.\n",
    "# Ans=\n",
    "Sure! Let's consider a simple example to illustrate the Apriori algorithm for learning association rules. Suppose we have a transactional dataset representing customer purchases in a grocery store. Here is a sample dataset:\n",
    "\n",
    "Transaction 1: Bread, Milk, Eggs\n",
    "Transaction 2: Bread, Juice\n",
    "Transaction 3: Milk, Juice\n",
    "Transaction 4: Bread, Milk, Juice\n",
    "Transaction 5: Bread, Eggs\n",
    "\n",
    "The Apriori algorithm follows these steps:\n",
    "\n",
    "Step 1: Set minimum support threshold\n",
    "Let's assume our minimum support threshold is 2, meaning that an itemset must appear in at least 2 transactions to be considered frequent.\n",
    "\n",
    "Step 2: Generate frequent itemsets of length 1\n",
    "Count the occurrence of each individual item in the dataset. Based on the minimum support threshold, the frequent itemsets of length 1 are:\n",
    "\n",
    "{Bread}, {Milk}, {Eggs}, {Juice}\n",
    "\n",
    "Step 3: Generate frequent itemsets of length 2\n",
    "Using the frequent itemsets of length 1, generate possible combinations of itemsets of length 2 and count their occurrences in the dataset. The frequent itemsets of length 2 are:\n",
    "\n",
    "{Bread, Milk}, {Bread, Eggs}, {Milk, Juice}\n",
    "\n",
    "Step 4: Generate frequent itemsets of length 3\n",
    "Again, using the frequent itemsets of length 2, generate possible combinations of itemsets of length 3 and count their occurrences in the dataset. The frequent itemsets of length 3 are:\n",
    "\n",
    "{Bread, Milk, Juice}\n",
    "\n",
    "Step 5: Generate association rules\n",
    "Using the frequent itemsets, generate association rules by considering different combinations of antecedents and consequents. For example, from the frequent itemset {Bread, Milk, Juice}, we can generate the following association rules:\n",
    "\n",
    "{Bread, Milk} -> {Juice}\n",
    "{Bread, Juice} -> {Milk}\n",
    "{Milk, Juice} -> {Bread}\n",
    "\n",
    "Step 6: Evaluate support and confidence\n",
    "Calculate the support and confidence for each association rule. Support measures the frequency of occurrence of the rule in the dataset, and confidence measures the conditional probability of the consequent given the antecedent.\n",
    "\n",
    "For example, if the rule {Bread, Milk} -> {Juice} has a support of 3 and a confidence of 75%, it means that the combination of Bread and Milk appears in 3 transactions, and in 75% of those transactions, Juice is also present.\n",
    "\n",
    "The Apriori algorithm continues to generate frequent itemsets of higher lengths and association rules until no more frequent itemsets can be generated based on the minimum support threshold.\n",
    "\n",
    "By applying the Apriori algorithm, we can discover frequent itemsets and association rules that provide insights into the relationships between items in the transactional dataset. These rules can then be used to make informed business decisions, such as product recommendations or targeted marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "# is used to decide when to end the iteration.\n",
    "# Ans=\n",
    "In hierarchical clustering, the distance between clusters is measured using a distance metric, which quantifies the dissimilarity or similarity between two clusters. The choice of distance metric depends on the nature of the data and the problem at hand. Some commonly used distance metrics include Euclidean distance, Manhattan distance, and cosine distance.\n",
    "\n",
    "The iteration in hierarchical clustering continues until all data points are assigned to a single cluster or until a stopping criterion is met. The stopping criterion is typically determined by defining a threshold or cutoff distance. This threshold specifies the maximum dissimilarity allowed for merging clusters.\n",
    "\n",
    "There are two main approaches to hierarchical clustering:\n",
    "\n",
    "Agglomerative (bottom-up) clustering:\n",
    "\n",
    "Start with each data point as an individual cluster.\n",
    "Calculate the pairwise distances between all clusters.\n",
    "Merge the two closest clusters based on the distance metric.\n",
    "Recalculate the pairwise distances between the merged cluster and the remaining clusters.\n",
    "Repeat the merging and distance calculation steps until the desired number of clusters is reached or until the clusters are merged into a single cluster.\n",
    "Divisive (top-down) clustering:\n",
    "\n",
    "Start with all data points in a single cluster.\n",
    "Calculate the distances between data points within the cluster.\n",
    "Split the cluster into two subclusters based on the distance metric.\n",
    "Recalculate the distances within each subcluster.\n",
    "Repeat the splitting and distance calculation steps until the desired number of clusters is reached or until each data point forms its own cluster.\n",
    "The decision to end the iteration and determine the number of clusters depends on the distance metric and the threshold set for the cutoff distance. As the clustering algorithm progresses, the dissimilarity between clusters increases. The stopping criterion can be defined based on a fixed cutoff distance or by using clustering evaluation metrics such as the silhouette coefficient or the gap statistic.\n",
    "\n",
    "Once the iteration is completed and the desired number of clusters is determined, the hierarchical clustering algorithm provides a hierarchical structure in the form of a dendrogram, which represents the merging or splitting of clusters at each step. The dendrogram can be visually analyzed to decide the appropriate number of clusters based on the problem domain or by using methods like the elbow method or silhouette analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "# Ans=\n",
    "In the k-means algorithm, the cluster centroids are recomputed in the following steps:\n",
    "\n",
    "Initialization: Randomly select K data points from the dataset as initial centroids.\n",
    "\n",
    "Assignment Step: For each data point in the dataset, calculate its distance to each centroid. Assign the data point to the cluster whose centroid is closest to it.\n",
    "\n",
    "Recomputation Step: Once all data points are assigned to clusters, compute the new centroids for each cluster based on the assigned data points. This is done by taking the mean of the feature values of all data points belonging to a particular cluster. The mean value represents the new centroid of that cluster.\n",
    "\n",
    "Convergence Check: Repeat the Assignment and Recomputation steps until the centroids no longer change significantly or until a maximum number of iterations is reached. The algorithm converges when the centroids stabilize and the data points are consistently assigned to the same clusters.\n",
    "\n",
    "The process of recomputing the cluster centroids is crucial in k-means as it updates the center of each cluster to better represent the data points within that cluster. The mean value of the feature values of the assigned data points is used as a representative point, which minimizes the sum of squared distances between the data points and their respective centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "# clusters.\n",
    "# Ans\n",
    "Determining the optimal number of clusters is an important task in clustering analysis. One method for determining the required number of clusters is the Elbow Method.\n",
    "\n",
    "The Elbow Method involves plotting the Within-Cluster Sum of Squares (WCSS) against the number of clusters. WCSS represents the sum of squared distances between each data point and its centroid within a cluster. The idea behind the Elbow Method is to find the point on the plot where the rate of decrease in WCSS significantly slows down, resulting in an \"elbow\" shape.\n",
    "\n",
    "The steps to determine the required number of clusters using the Elbow Method are as follows:\n",
    "\n",
    "Compute the clustering algorithm (e.g., k-means) for a range of cluster numbers, starting from 1 to a predefined maximum number of clusters.\n",
    "\n",
    "For each number of clusters, calculate the WCSS.\n",
    "\n",
    "Plot the number of clusters on the x-axis and the corresponding WCSS on the y-axis.\n",
    "\n",
    "Examine the resulting plot and look for the point where adding more clusters does not significantly decrease the WCSS. This point forms an \"elbow\" shape on the plot.\n",
    "\n",
    "The number of clusters corresponding to the elbow point is considered as the optimal number of clusters for the given dataset.\n",
    "\n",
    "It's important to note that the Elbow Method is subjective and may require domain knowledge and interpretation. Sometimes, the elbow point may not be distinct, and other methods or domain-specific considerations may be necessary to determine the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "# Ans=\n",
    "The k-means algorithm has several advantages and disadvantages:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simplicity: The k-means algorithm is relatively simple to understand and implement. It follows a straightforward iterative process, making it accessible to users with basic knowledge of clustering techniques.\n",
    "\n",
    "Scalability: K-means can efficiently handle large datasets with a moderate number of features. Its time complexity is linear with respect to the number of data points, making it suitable for applications with a substantial number of observations.\n",
    "\n",
    "Interpretability: The resulting clusters from k-means are easily interpretable. Each cluster is represented by its centroid, which serves as a prototype for the cluster. This centroid can provide insights into the characteristics of the cluster.\n",
    "\n",
    "Speed: Due to its simplicity and efficiency, k-means can converge relatively quickly, especially when compared to more complex clustering algorithms. This makes it a favorable choice for large-scale applications where computational time is a concern.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Sensitivity to initial centroids: The initial placement of centroids in k-means can influence the final clustering result. Different initializations can lead to different cluster assignments and may result in suboptimal solutions. Running k-means multiple times with different initializations can mitigate this issue.\n",
    "\n",
    "Difficulty handling non-linear data: K-means assumes that clusters are spherical and have similar sizes. It may struggle with datasets that have complex non-linear structures or clusters with varying shapes and densities. In such cases, other clustering algorithms, such as density-based clustering or hierarchical clustering, may be more suitable.\n",
    "\n",
    "Need to specify the number of clusters: K-means requires the user to specify the desired number of clusters in advance. Determining the appropriate number of clusters is often a challenging task, and an incorrect choice can lead to suboptimal clustering results. Various techniques, such as the Elbow Method or silhouette analysis, can help estimate the optimal number of clusters.\n",
    "\n",
    "Sensitivity to outliers: K-means can be sensitive to outliers, as they can significantly affect the position of cluster centroids. Outliers can distort the cluster assignments and lead to inaccurate clustering results. Preprocessing techniques, such as outlier detection or data normalization, can help mitigate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Draw a diagram to demonstrate the principle of clustering.\n",
    "Ans=As a text-based AI, I'm unable to draw diagrams directly. However, I can describe the principle of clustering for you.\n",
    "\n",
    "Clustering is a technique used in unsupervised learning to group similar data points together based on their inherent patterns or similarities. The goal is to partition the data into distinct clusters, where data points within the same cluster are more similar to each other than to those in other clusters.\n",
    "\n",
    "To illustrate the principle of clustering, imagine a scatter plot with data points representing objects in a two-dimensional space. The data points can be represented as dots, and the x and y axes represent different features or variables.\n",
    "\n",
    "In clustering, the algorithm identifies clusters by considering the proximity or distance between data points. Points that are close together in the feature space are assigned to the same cluster. The clusters are represented by different colors or shapes in the diagram.\n",
    "\n",
    "The clustering process aims to minimize the distance between data points within the same cluster (intra-cluster distance) while maximizing the distance between different clusters (inter-cluster distance). This ensures that the clusters are compact and well-separated.\n",
    "\n",
    "The diagram would show distinct groups or clusters formed by the clustering algorithm, where points within each cluster are close together, and points between different clusters are far apart. The clusters may have different shapes, sizes, or densities depending on the characteristics of the data.\n",
    "\n",
    "Keep in mind that the specific layout and configuration of the clusters would depend on the data and the clustering algorithm used. Different clustering algorithms may produce different results, and the choice of algorithm should align with the nature of the data and the clustering goals.\n",
    "\n",
    "It's recommended to use visualization tools or software packages specifically designed for clustering analysis to generate accurate and visually appealing diagrams that depict the clustering principle effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "# the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "# C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "# C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "# C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "# C3: (5,5) and (9,9)\n",
    "# Ans=\n",
    "\n",
    "To build three clusters using the K-means algorithm, we start with an initial assignment of data points to clusters and iteratively update the cluster centroids until convergence. Let's go through the steps for the given data points:\n",
    "\n",
    "Initialize the cluster centroids:\n",
    "\n",
    "C1: (2, 2)\n",
    "C2: (4, 4)\n",
    "C3: (6, 6)\n",
    "Assign each data point to the closest centroid:\n",
    "\n",
    "(2, 2) is closest to C1\n",
    "(4, 4) is equidistant to C1 and C2, so we assign it to C1 (or C2)\n",
    "(6, 6) is closest to C3\n",
    "(0, 4) is closest to C2\n",
    "(4, 0) is equidistant to C1 and C2, so we assign it to C1 (or C2)\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(0, 4) is closest to C2\n",
    "(5, 5) is closest to C3\n",
    "(9, 9) is closest to C3\n",
    "Update the cluster centroids:\n",
    "\n",
    "C1: (2, 2), (4, 4), (4, 0) (average: (10/3, 6/3) = (3.33, 2))\n",
    "C2: (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4) (average: (0, 4))\n",
    "C3: (6, 6), (5, 5), (9, 9) (average: (20/3, 20/3) = (6.67, 6.67))\n",
    "At this point, we have completed one iteration of the K-means algorithm. We can repeat steps 2 and 3 until convergence, where the cluster assignments and centroids no longer change significantly.\n",
    "\n",
    "Note: In step 2, the equidistant data points can be assigned to either of the centroids. The final clustering result may vary depending on the initial centroid positions and the order in which the data points are processed.\n",
    "\n",
    "Please let me know if you need further clarification or if you have any additional questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. In a software project, the team is attempting to determine if software flaws discovered during\n",
    "# testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters\n",
    "# of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
    "# be listed as one of the forms identified by clustering. A simple diagram can be used to explain this\n",
    "# process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the\n",
    "# k-means algorithm.\n",
    "# Ans=\n",
    "Certainly! Let's represent the process of clustering 20 defect data points into 5 clusters using a simple diagram:\n",
    "\n",
    "yaml\n",
    "Copy code\n",
    "Defect Data Points: 20\n",
    "Clusters: 5\n",
    "\n",
    "Initial Random Centroids:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Iteration 1:\n",
    "Assign Data Points to Nearest Centroid:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Update Centroids:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Iteration 2:\n",
    "Assign Data Points to Nearest Centroid:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Update Centroids:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Iteration 3:\n",
    "Assign Data Points to Nearest Centroid:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Update Centroids:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Iteration 4:\n",
    "Assign Data Points to Nearest Centroid:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Update Centroids:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Iteration 5:\n",
    "Assign Data Points to Nearest Centroid:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "\n",
    "Update Centroids:\n",
    "C1: \n",
    "C2: \n",
    "C3: \n",
    "C4: \n",
    "C5: \n",
    "In the diagram, the initial random centroids for each cluster are represented by C1, C2, C3, C4, and C5. In each iteration, the data points are assigned to the nearest centroid, and then the centroids are updated based on the assigned data points. The process continues until convergence, where the cluster assignments and centroids no longer change significantly.\n",
    "\n",
    "Please note that the actual data points and their cluster assignments and centroids are not specified in the diagram since it depends on the specific data and the results of the k-means algorithm.\n",
    "\n",
    "Let me know if you have any further questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
